---
phase: 00-foundation-ai-architecture
plan: 02
type: execute
wave: 2
depends_on: ["00-01"]
files_modified:
  - src/lib/ai/providers.ts
  - src/lib/ai/models.ts
  - src/lib/ai/index.ts
  - src/services/ai/orchestrator.ts
  - src/app/api/test-ai/route.ts
  - src/app/page.tsx
autonomous: false
user_setup:
  - service: anthropic
    why: "Claude dla mentoringu i dlugiego kontekstu"
    env_vars:
      - name: ANTHROPIC_API_KEY
        source: "https://console.anthropic.com/ -> API Keys"
  - service: openai
    why: "GPT dla structured outputs (curriculum)"
    env_vars:
      - name: OPENAI_API_KEY
        source: "https://platform.openai.com/api-keys"
  - service: google
    why: "Gemini Flash dla szybkich quizow"
    env_vars:
      - name: GOOGLE_GENERATIVE_AI_API_KEY
        source: "https://aistudio.google.com/apikey"

must_haves:
  truths:
    - "Endpoint /api/test-ai zwraca streaming response"
    - "Logi w konsoli pokazuja zuzycie tokenow (inputTokens, outputTokens)"
    - "MODEL_CONFIG routuje rozne zadania do roznych modeli"
    - "Provider registry obsluguje trzy providery (anthropic, openai, google)"
  artifacts:
    - path: "src/lib/ai/providers.ts"
      provides: "Provider registry i MODEL_CONFIG"
      exports: ["registry", "MODEL_CONFIG", "getModel"]
    - path: "src/services/ai/orchestrator.ts"
      provides: "AI orchestration z cost tracking"
      exports: ["executeAITask", "getCostLogs"]
    - path: "src/app/api/test-ai/route.ts"
      provides: "Streaming test endpoint"
      exports: ["GET"]
  key_links:
    - from: "src/app/api/test-ai/route.ts"
      to: "src/lib/ai/providers.ts"
      via: "getModel import"
      pattern: "import.*getModel.*from.*providers"
    - from: "src/services/ai/orchestrator.ts"
      to: "src/lib/ai/providers.ts"
      via: "MODEL_CONFIG import"
      pattern: "import.*MODEL_CONFIG.*from.*providers"
---

<objective>
Zbudowanie warstwy multi-model AI orchestration z Provider Registry, routingiem modeli i monitoringiem kosztow.

Purpose: Umozliwienie inteligentnego routingu zadan AI do odpowiednich modeli (Claude dla mentoringu, GPT dla curriculum, Gemini dla quizow) z centralnym trackingiem kosztow od pierwszego dnia.

Output: Dzialajacy endpoint /api/test-ai demonstrujacy streaming response z cost logging.
</objective>

<execution_context>
@C:\Users\dariu\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\dariu\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/00-foundation-ai-architecture/00-RESEARCH.md
@.planning/phases/00-foundation-ai-architecture/00-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Provider Registry i Model Configuration</name>
  <files>
    src/lib/ai/providers.ts
    src/lib/ai/models.ts
    src/lib/ai/index.ts
  </files>
  <action>
    1. Utworz src/lib/ai/providers.ts - centralna konfiguracja providerow:
       ```typescript
       // Provider Registry for Multi-Model AI
       // Source: https://ai-sdk.dev/docs/ai-sdk-core/provider-management

       import { anthropic } from '@ai-sdk/anthropic';
       import { openai } from '@ai-sdk/openai';
       import { google } from '@ai-sdk/google';
       import { experimental_createProviderRegistry as createProviderRegistry } from 'ai';

       // Model routing configuration - rozne modele do roznych zadan
       // Koszty optymalizowane: drogi model tylko tam gdzie potrzeba jakosci
       export const MODEL_CONFIG = {
         // Claude Sonnet 4 - mentoring, long context, empathy, Polish language
         mentor: anthropic('claude-sonnet-4-20250514'),

         // GPT-4.1 - structured curriculum generation, reliable JSON
         curriculum: openai('gpt-4.1'),

         // Gemini 2.0 Flash - fast, cheap quizzes
         quiz: google('gemini-2.0-flash'),

         // Embeddings for RAG (Phase 5)
         embedding: openai('text-embedding-3-small'),
       } as const;

       // Provider registry with all configured providers
       export const registry = createProviderRegistry({
         anthropic,
         openai,
         google,
       });

       // Helper to get model for specific task
       export function getModel(task: keyof typeof MODEL_CONFIG) {
         return MODEL_CONFIG[task];
       }

       // Get model name string for logging
       export function getModelName(task: keyof typeof MODEL_CONFIG): string {
         const model = MODEL_CONFIG[task];
         return model.modelId || task;
       }
       ```

    2. Utworz src/lib/ai/models.ts - dodatkowa konfiguracja modeli:
       ```typescript
       // Model-specific configurations and constraints

       export const MODEL_CONSTRAINTS = {
         mentor: {
           maxTokens: 4096,
           temperature: 0.7,  // More creative for engaging conversation
           systemPrompt: `Jestes Wirtualnym Mentorem - przyjaznym nauczycielem AI.
       Uzywasz metody sokratycznej: naprowadzasz na odpowiedz zamiast dawac gotowe rozwiazania.
       Mowisz po polsku, jasno i przystepnie.
       Wspierasz i motywujesz ucznia.`,
         },
         curriculum: {
           maxTokens: 8192,
           temperature: 0.3,  // More deterministic for structured output
         },
         quiz: {
           maxTokens: 2048,
           temperature: 0.5,
         },
       } as const;

       // Cost estimates per 1M tokens (for budgeting)
       // Source: Provider pricing pages, January 2025
       export const COST_PER_MILLION = {
         'claude-sonnet-4-20250514': { input: 3.0, output: 15.0 },
         'gpt-4.1': { input: 2.0, output: 8.0 },
         'gemini-2.0-flash': { input: 0.10, output: 0.40 },
         'text-embedding-3-small': { input: 0.02, output: 0 },
       } as const;
       ```

    3. Utworz src/lib/ai/index.ts - eksporty:
       ```typescript
       // AI Library exports
       export { MODEL_CONFIG, registry, getModel, getModelName } from './providers';
       export { MODEL_CONSTRAINTS, COST_PER_MILLION } from './models';
       ```

    WAZNE: Uzyj `experimental_createProviderRegistry` jesli `createProviderRegistry` nie istnieje (AI SDK moze miec to jako experimental). Sprawdz dokumentacje.
  </action>
  <verify>
    ```bash
    npx tsc --noEmit src/lib/ai/providers.ts src/lib/ai/models.ts src/lib/ai/index.ts
    ```
    Brak bledow kompilacji TypeScript.

    ```bash
    cat src/lib/ai/providers.ts | grep "MODEL_CONFIG"
    ```
    MODEL_CONFIG jest zdefiniowany.
  </verify>
  <done>
    - src/lib/ai/providers.ts zawiera MODEL_CONFIG z 4 modelami (mentor, curriculum, quiz, embedding)
    - src/lib/ai/providers.ts eksportuje getModel() helper
    - src/lib/ai/models.ts zawiera MODEL_CONSTRAINTS i COST_PER_MILLION
    - src/lib/ai/index.ts eksportuje wszystko z jednego miejsca
    - Kod kompiluje sie bez bledow TypeScript
  </done>
</task>

<task type="auto">
  <name>Task 2: AI Orchestrator z Cost Tracking</name>
  <files>
    src/services/ai/orchestrator.ts
  </files>
  <action>
    Utworz src/services/ai/orchestrator.ts - centralna logika orchestration:

    ```typescript
    // AI Orchestrator - centralny punkt dla wszystkich wywolan AI
    // Zapewnia: model routing, cost tracking, error handling

    import { generateText, streamText, generateObject, LanguageModelUsage } from 'ai';
    import { z } from 'zod';
    import { getModel, getModelName } from '@/lib/ai/providers';
    import { MODEL_CONSTRAINTS, COST_PER_MILLION } from '@/lib/ai/models';
    import type { AITask, CostLog } from '@/types/ai';

    // In-memory cost logs (replace with DB in production - Phase 7)
    const costLogs: CostLog[] = [];

    /**
     * Log token usage for cost tracking
     */
    function logUsage(
      task: AITask,
      modelName: string,
      usage: LanguageModelUsage,
      durationMs?: number
    ): void {
      const log: CostLog = {
        task,
        model: modelName,
        inputTokens: usage.promptTokens,
        outputTokens: usage.completionTokens,
        timestamp: new Date(),
        durationMs,
      };

      costLogs.push(log);

      // Console log in development
      if (process.env.NODE_ENV === 'development') {
        const costs = COST_PER_MILLION[modelName as keyof typeof COST_PER_MILLION];
        const estimatedCost = costs
          ? (usage.promptTokens * costs.input + usage.completionTokens * costs.output) / 1_000_000
          : 0;

        console.log(
          `[AI COST] ${task} (${modelName}): ` +
          `${usage.promptTokens} in, ${usage.completionTokens} out ` +
          `(~$${estimatedCost.toFixed(4)})`
        );
      }
    }

    /**
     * Execute AI task with automatic model routing and cost tracking
     */
    export async function executeAITask<T = string>(
      task: AITask,
      params: {
        prompt?: string;
        messages?: Array<{ role: 'user' | 'assistant' | 'system'; content: string }>;
        schema?: z.ZodSchema<T>;
        stream?: boolean;
        systemPrompt?: string;
      }
    ): Promise<T | ReturnType<typeof streamText>> {
      const model = getModel(task);
      const modelName = getModelName(task);
      const constraints = MODEL_CONSTRAINTS[task as keyof typeof MODEL_CONSTRAINTS];
      const startTime = Date.now();

      // Use task-specific system prompt if not overridden
      const systemPrompt = params.systemPrompt ?? constraints?.systemPrompt;

      // Structured output with Zod schema
      if (params.schema) {
        const result = await generateObject({
          model,
          schema: params.schema,
          prompt: params.prompt,
          system: systemPrompt,
          maxTokens: constraints?.maxTokens,
          temperature: constraints?.temperature,
        });

        logUsage(task, modelName, result.usage, Date.now() - startTime);
        return result.object as T;
      }

      // Streaming response
      if (params.stream) {
        const result = streamText({
          model,
          messages: params.messages,
          prompt: params.prompt,
          system: systemPrompt,
          maxTokens: constraints?.maxTokens,
          temperature: constraints?.temperature,
          onFinish: ({ usage }) => {
            logUsage(task, modelName, usage, Date.now() - startTime);
          },
        });

        return result as ReturnType<typeof streamText>;
      }

      // Simple text generation
      const result = await generateText({
        model,
        prompt: params.prompt,
        messages: params.messages,
        system: systemPrompt,
        maxTokens: constraints?.maxTokens,
        temperature: constraints?.temperature,
      });

      logUsage(task, modelName, result.usage, Date.now() - startTime);
      return result.text as T;
    }

    /**
     * Get all cost logs (for monitoring dashboard - Phase 7)
     */
    export function getCostLogs(): CostLog[] {
      return [...costLogs];
    }

    /**
     * Get cost summary by task
     */
    export function getCostSummary(): Record<AITask, { calls: number; totalTokens: number }> {
      const summary: Record<string, { calls: number; totalTokens: number }> = {};

      for (const log of costLogs) {
        if (!summary[log.task]) {
          summary[log.task] = { calls: 0, totalTokens: 0 };
        }
        summary[log.task].calls++;
        summary[log.task].totalTokens += log.inputTokens + log.outputTokens;
      }

      return summary as Record<AITask, { calls: number; totalTokens: number }>;
    }

    /**
     * Clear cost logs (for testing)
     */
    export function clearCostLogs(): void {
      costLogs.length = 0;
    }
    ```

    UWAGA: Uzywamy `LanguageModelUsage` dla typowania usage object. Sprawdz czy to poprawny import z 'ai'.
  </action>
  <verify>
    ```bash
    npx tsc --noEmit src/services/ai/orchestrator.ts
    ```
    Brak bledow kompilacji.

    ```bash
    grep -n "logUsage" src/services/ai/orchestrator.ts
    ```
    Funkcja logUsage jest zdefiniowana i uzywana.
  </verify>
  <done>
    - src/services/ai/orchestrator.ts eksportuje executeAITask, getCostLogs, getCostSummary
    - Cost tracking loguje kazde wywolanie AI z tokenami i szacowanym kosztem
    - Orchestrator automatycznie wybiera model na podstawie task type
    - System prompts sa aplikowane z MODEL_CONSTRAINTS
    - Kod kompiluje sie bez bledow
  </done>
</task>

<task type="auto">
  <name>Task 3: Test Endpoint i aktualizacja strony</name>
  <files>
    src/app/api/test-ai/route.ts
    src/app/page.tsx
  </files>
  <action>
    1. Utworz src/app/api/test-ai/route.ts - streaming test endpoint:
       ```typescript
       // Test endpoint for AI SDK verification
       // GET /api/test-ai - returns streaming response from mentor model

       import { streamText } from 'ai';
       import { getModel } from '@/lib/ai';

       export async function GET() {
         const result = streamText({
           model: getModel('mentor'),
           prompt: 'Przedstaw sie krotko jako Wirtualny Mentor. Powiedz czym sie zajmujesz i jak mozesz pomoc w nauce. Odpowiedz po polsku, max 3 zdania.',
         });

         return result.toTextStreamResponse();
       }

       // Edge runtime for better streaming performance
       export const runtime = 'edge';
       ```

    2. Zaktualizuj src/app/page.tsx - dodaj przycisk testowy:
       ```typescript
       'use client';

       import { useState } from 'react';

       export default function Home() {
         const [response, setResponse] = useState<string>('');
         const [loading, setLoading] = useState(false);
         const [error, setError] = useState<string | null>(null);

         async function testAI() {
           setLoading(true);
           setError(null);
           setResponse('');

           try {
             const res = await fetch('/api/test-ai');

             if (!res.ok) {
               throw new Error(`HTTP ${res.status}: ${res.statusText}`);
             }

             const reader = res.body?.getReader();
             if (!reader) throw new Error('No response body');

             const decoder = new TextDecoder();

             while (true) {
               const { done, value } = await reader.read();
               if (done) break;

               const chunk = decoder.decode(value, { stream: true });
               setResponse(prev => prev + chunk);
             }
           } catch (err) {
             setError(err instanceof Error ? err.message : 'Unknown error');
           } finally {
             setLoading(false);
           }
         }

         return (
           <main className="min-h-screen bg-gradient-to-b from-gray-900 to-gray-800 text-white">
             <div className="container mx-auto px-4 py-16 max-w-2xl">
               <div className="text-center mb-12">
                 <h1 className="text-4xl font-bold mb-4">
                   Wirtualny Mentor
                 </h1>
                 <p className="text-xl text-gray-300">
                   Spersonalizowana platforma nauki z AI
                 </p>
                 <p className="text-sm text-gray-500 mt-2">
                   Phase 0: Foundation & AI Architecture
                 </p>
               </div>

               <div className="bg-gray-800 rounded-lg p-6 shadow-xl">
                 <h2 className="text-lg font-semibold mb-4">Test AI Connection</h2>

                 <button
                   onClick={testAI}
                   disabled={loading}
                   className="w-full bg-blue-600 hover:bg-blue-700 disabled:bg-gray-600
                            text-white font-medium py-3 px-6 rounded-lg
                            transition-colors duration-200"
                 >
                   {loading ? 'AI odpowiada...' : 'Przetestuj AI'}
                 </button>

                 {error && (
                   <div className="mt-4 p-4 bg-red-900/50 border border-red-500 rounded-lg">
                     <p className="text-red-300">{error}</p>
                     <p className="text-sm text-red-400 mt-2">
                       Sprawdz czy klucze API sa skonfigurowane w .env.local
                     </p>
                   </div>
                 )}

                 {response && (
                   <div className="mt-4 p-4 bg-gray-700 rounded-lg">
                     <p className="text-sm text-gray-400 mb-2">Odpowiedz AI:</p>
                     <p className="text-gray-100 whitespace-pre-wrap">{response}</p>
                   </div>
                 )}
               </div>

               <p className="text-center text-gray-500 text-sm mt-8">
                 Kliknij przycisk powyzej aby przetestowac polaczenie z AI.
                 <br />
                 Wymaga skonfigurowanych kluczy API w .env.local
               </p>
             </div>
           </main>
         );
       }
       ```

    3. Utworz plik src/lib/ai/index.ts jesli nie istnieje (re-export):
       ```typescript
       export * from './providers';
       export * from './models';
       ```
  </action>
  <verify>
    ```bash
    npm run dev
    ```
    Aplikacja uruchamia sie bez bledow.

    W przegladarce: Otworz http://localhost:3000, kliknij "Przetestuj AI".
    - Jesli klucze API skonfigurowane: powinna pojawic sie streaming odpowiedz
    - Jesli brak kluczy: powinna pojawic sie informacja o bledzie

    W konsoli serwera: Powinien pojawic sie log [AI COST] z tokenami.
  </verify>
  <done>
    - /api/test-ai endpoint istnieje i uzywa edge runtime
    - Strona glowna ma przycisk "Przetestuj AI"
    - Streaming response wyswietla sie w UI
    - Bledy sa wyswietlane czytelnie (np. brak kluczy API)
    - Konsola loguje zuzycie tokenow
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
    Multi-model AI orchestration layer z:
    - Provider Registry (Claude, GPT, Gemini)
    - MODEL_CONFIG z routingiem zadan do modeli
    - Cost tracking z logami w konsoli
    - Streaming test endpoint /api/test-ai
    - UI do testowania polaczenia AI
  </what-built>
  <how-to-verify>
    1. Skopiuj .env.example do .env.local
    2. Dodaj co najmniej jeden klucz API (najlepiej ANTHROPIC_API_KEY dla testu mentora)
    3. Uruchom `npm run dev`
    4. Otworz http://localhost:3000
    5. Kliknij przycisk "Przetestuj AI"
    6. Sprawdz:
       - [ ] Odpowiedz streamuje sie (pojawia sie stopniowo, nie cala naraz)
       - [ ] Odpowiedz jest po polsku
       - [ ] W konsoli serwera widac log typu: [AI COST] mentor (claude-sonnet-4-...): X in, Y out (~$0.00XX)
    7. Jesli brak klucza API, sprawdz czy wyswietla sie czytelny komunikat bledu
  </how-to-verify>
  <resume-signal>
    Wpisz "approved" jesli wszystko dziala poprawnie.
    Jesli sa problemy, opisz co nie dziala.
  </resume-signal>
</task>

</tasks>

<verification>
1. src/lib/ai/providers.ts eksportuje MODEL_CONFIG, registry, getModel
2. src/services/ai/orchestrator.ts eksportuje executeAITask, getCostLogs
3. /api/test-ai endpoint zwraca streaming response
4. Konsola loguje zuzycie tokenow przy kazdym wywolaniu AI
5. UI wyswietla streaming response lub czytelny blad
6. TypeScript kompiluje sie bez bledow
</verification>

<success_criteria>
- Trzy providery AI (Anthropic, OpenAI, Google) skonfigurowane w registry
- MODEL_CONFIG routuje zadania: mentor->Claude, curriculum->GPT, quiz->Gemini
- Kazde wywolanie AI loguje tokeny i szacowany koszt
- Endpoint /api/test-ai demonstruje dzialajace streaming
- User potwierdza ze AI odpowiada po polsku przez UI
</success_criteria>

<output>
Po zakonczeniu utworz `.planning/phases/00-foundation-ai-architecture/00-02-SUMMARY.md` wedlug wzorca z @summary.md
</output>
